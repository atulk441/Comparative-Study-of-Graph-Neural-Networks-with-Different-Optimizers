# Comparative-Study-of-Graph-Neural-Networks-with-Different-Optimizers

## Abstract
This project examines the performance of several Graph Neural Network (GNN) versions, such as Graph Autoencoders, Graph Convolutional Networks, Graph Attention Networks, and Graph Sample and Aggregation. We assess the impact of several optimization strategies, including ASGD, LBFGS, RMSprop, and advanced approaches like as Adam, RAdam, NAdam, and SNRAdam, on these models. The purpose of the study is to determine how each optimizer affects training patterns and model accuracy. Our findings will provide useful insights into improving GNNs for better performance in graph-based work.

## Highlights of Research
1) Engineered Graph Autoencoders, GCNs, GATs, and Graph SAGE models using PyTorch, optimizing for performance and scalability.
2) Evaluated both traditional (e.g., ASGD, LBFGS) and advanced (e.g., Adam, Adagrad, Adadelta, RAdam, NAdam, AdamW, and SNRAdam) optimization algorithms.
3) Discovered significant variations in optimizer performance across models and datasets, providing insights for improving accuracy and training efficiency.
